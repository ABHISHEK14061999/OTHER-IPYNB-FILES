{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Call_Backs_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQDRNrY2NCXf"
      },
      "source": [
        "<pre>\n",
        "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
        "\n",
        "2. Code the model to classify data like below image\n",
        "\n",
        "<img src='https://i.imgur.com/33ptOFy.png'>\n",
        "\n",
        "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
        "\n",
        "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
        "\n",
        "5. you have to decay learning based on below conditions \n",
        "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
        "               learning rate by 10%. \n",
        "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
        "        \n",
        "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
        "\n",
        "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
        "\n",
        "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
        "\n",
        "9. use cross entropy as loss function\n",
        "\n",
        "10. Try the architecture params as given below. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41Y3TFENCXk"
      },
      "source": [
        "<pre>\n",
        "<b>Model-1</b>\n",
        "<pre>\n",
        "1. Use tanh as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use RandomUniform(0,1) as initilizer.\n",
        "3. Analyze your output and training process. \n",
        "</pre>\n",
        "</pre>\n",
        "<pre>\n",
        "<b>Model-2</b>\n",
        "<pre>\n",
        "1. Use relu as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use RandomUniform(0,1) as initilizer.\n",
        "3. Analyze your output and training process. \n",
        "</pre>\n",
        "</pre>\n",
        "<pre>\n",
        "<b>Model-3</b>\n",
        "<pre>\n",
        "1. Use relu as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use he_uniform() as initilizer.\n",
        "3. Analyze your output and training process. \n",
        "</pre>\n",
        "</pre>\n",
        "<pre>\n",
        "<b>Model-4</b>\n",
        "<pre>\n",
        "1. Try with any values to get better accuracy/f1 score.  \n",
        "</pre>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA8ls8TGi1Nl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "68f5b253-f7cc-4a9f-a362-30bbc592e72d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "data=pd.read_csv('/content/drive/My Drive/Colab Notebooks/data.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.450564</td>\n",
              "      <td>1.074305</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.085632</td>\n",
              "      <td>0.967682</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.117326</td>\n",
              "      <td>0.971521</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.982179</td>\n",
              "      <td>-0.380408</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.720352</td>\n",
              "      <td>0.955850</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2  label\n",
              "0  0.450564  1.074305    0.0\n",
              "1  0.085632  0.967682    0.0\n",
              "2  0.117326  0.971521    1.0\n",
              "3  0.982179 -0.380408    0.0\n",
              "4 -0.720352  0.955850    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_spWcvqjxOS"
      },
      "source": [
        "#2.1 Callback function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSLjZVKxj5FA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b5b4830d-1db5-4db5-fef6-3cb8a5aee2d8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense,Input,Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import random as rn\n",
        "\n",
        "X = data.drop(['label'], axis=1).values\n",
        "Y = data['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, stratify=Y) \n",
        "#X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13400, 2)\n",
            "(13400,)\n",
            "(6600, 2)\n",
            "(6600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCxVpwXRRqQa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "#from keras.callbacks import Callback\n",
        "\n",
        "'''def getf1(X_test,y_test):\n",
        "  val_f1 = []\n",
        "  y_pred = model.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred,average='micro')\n",
        "  val_f1.append(f1)\n",
        "  print(val_f1)'''\n",
        "\n",
        "class Metrics(tf.keras.callbacks.Callback):\n",
        "\n",
        "   def on_train_begin(self, logs={}):\n",
        "     self.val_f1s = []\n",
        "     \n",
        "\n",
        "   def on_epoch_end(self, epoch, logs={}):\n",
        "     #val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
        "     val_predict = (np.asarray(self.model.predict(X_test))).round()\n",
        "     #val_targ = self.model.validation_data[1]\n",
        "     _val_f1 = f1_score(y_test, val_predict,average='micro') \n",
        "     self.val_f1s.append(_val_f1)\n",
        "     #print(\" value f1 \",_val_f1)\n",
        "     print(\"  f1_score: \"+\"{:.4f}\".format(_val_f1)); \n",
        "     \n",
        "     return\n",
        "\n",
        "history_own=Metrics()\n",
        "#print(history_own.val_f1s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TlQkEH827Cp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8ade87bc-b00d-40ae-e5a1-9d11d2d82d24"
      },
      "source": [
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(layer1)\n",
        "\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "\n",
        "#Callbacks\n",
        "#history_own = LossHistory()\n",
        "history_own  = Metrics()\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=5, validation_data=(X_test,y_test), batch_size=20, callbacks=[history_own])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "636/670 [===========================>..] - ETA: 0s - loss: 0.7187 - auc: 0.5004  f1_score: 0.5041\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.7175 - auc: 0.5006 - val_loss: 0.6958 - val_auc: 0.4936\n",
            "Epoch 2/5\n",
            "664/670 [============================>.] - ETA: 0s - loss: 0.6924 - auc: 0.5105  f1_score: 0.5324\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6924 - auc: 0.5105 - val_loss: 0.6917 - val_auc: 0.5099\n",
            "Epoch 3/5\n",
            "667/670 [============================>.] - ETA: 0s - loss: 0.6908 - auc: 0.5647  f1_score: 0.5385\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6908 - auc: 0.5650 - val_loss: 0.6906 - val_auc: 0.5706\n",
            "Epoch 4/5\n",
            "654/670 [============================>.] - ETA: 0s - loss: 0.6897 - auc: 0.5793  f1_score: 0.5868\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6897 - auc: 0.5794 - val_loss: 0.6895 - val_auc: 0.5922\n",
            "Epoch 5/5\n",
            "637/670 [===========================>..] - ETA: 0s - loss: 0.6887 - auc: 0.5943  f1_score: 0.5817\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6886 - auc: 0.5950 - val_loss: 0.6886 - val_auc: 0.5940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4de255a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEvrC_ynlnP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e80fd31a-e8c3-4403-80fe-cbe83f4264e6"
      },
      "source": [
        "history_own.val_f1s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5040909090909091,\n",
              " 0.5324242424242425,\n",
              " 0.5384848484848485,\n",
              " 0.5868181818181818,\n",
              " 0.5816666666666667]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_rwmgWcvIqq"
      },
      "source": [
        "## 2.2 Termiate training If getting any NaN values(either weigths or loss) while training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT2v8Q17vLYh"
      },
      "source": [
        "class TerminateNaN(tf.keras.callbacks.Callback):\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        loss = logs.get('loss')\n",
        "        if loss is not None:\n",
        "            if np.isnan(loss) or np.isinf(loss):\n",
        "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
        "                self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QbZKrAsJZVJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1dad0ca8-ac8f-45ad-a3f4-1bbd5f9c3cec"
      },
      "source": [
        "terminate= TerminateNaN()\n",
        "\n",
        "model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test),batch_size=20,callbacks=[terminate])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6875 - auc: 0.5998 - val_loss: 0.6876 - val_auc: 0.5933\n",
            "Epoch 2/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6864 - auc: 0.6012 - val_loss: 0.6866 - val_auc: 0.5941\n",
            "Epoch 3/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6854 - auc: 0.6031 - val_loss: 0.6855 - val_auc: 0.5978\n",
            "Epoch 4/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6842 - auc: 0.6039 - val_loss: 0.6845 - val_auc: 0.5987\n",
            "Epoch 5/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6832 - auc: 0.6046 - val_loss: 0.6835 - val_auc: 0.5982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4ddd573c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g6sd5R4xhTt"
      },
      "source": [
        "## 2.3 ModelCheckpoint - Save the model after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBxkJA64FBr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "fd7449cf-dd13-4596-949a-5068e48e6f34"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(layer1)\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "\n",
        "#Callbacks\n",
        "#file path, it saves the model in the 'model_save' folder and we are naming model with epoch number \n",
        "#and val auc to differtiate with other models\n",
        "#you have to create model_save folder before running the code.\n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_auc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_auc',  verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test),batch_size=20,callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "641/670 [===========================>..] - ETA: 0s - loss: 0.6989 - auc: 0.4993\n",
            "Epoch 00001: val_auc improved from -inf to 0.49541, saving model to model_save/weights-01-0.50.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6987 - auc: 0.4997 - val_loss: 0.6944 - val_auc: 0.4954\n",
            "Epoch 2/5\n",
            "654/670 [============================>.] - ETA: 0s - loss: 0.6934 - auc: 0.4974\n",
            "Epoch 00002: val_auc improved from 0.49541 to 0.50885, saving model to model_save/weights-02-0.51.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6934 - auc: 0.4981 - val_loss: 0.6931 - val_auc: 0.5088\n",
            "Epoch 3/5\n",
            "649/670 [============================>.] - ETA: 0s - loss: 0.6931 - auc: 0.5050\n",
            "Epoch 00003: val_auc did not improve from 0.50885\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6931 - auc: 0.5044 - val_loss: 0.6929 - val_auc: 0.5035\n",
            "Epoch 4/5\n",
            "645/670 [===========================>..] - ETA: 0s - loss: 0.6929 - auc: 0.5077\n",
            "Epoch 00004: val_auc improved from 0.50885 to 0.51983, saving model to model_save/weights-04-0.52.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6929 - auc: 0.5082 - val_loss: 0.6928 - val_auc: 0.5198\n",
            "Epoch 5/5\n",
            "641/670 [===========================>..] - ETA: 0s - loss: 0.6928 - auc: 0.5188\n",
            "Epoch 00005: val_auc improved from 0.51983 to 0.53409, saving model to model_save/weights-05-0.53.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6928 - auc: 0.5201 - val_loss: 0.6927 - val_auc: 0.5341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4ddf430eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUNpckcAMPtI"
      },
      "source": [
        "## 2.4 You have to stop the training if your validation accuracy is not increased in last 2 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aup2xgX4MhVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1ff1c70a-9a57-43d1-acbd-eacd87d3bc08"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(layer1)\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_auc', patience=2, verbose=1, mode='max')\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test),batch_size=20,callbacks=[earlystop])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.7010 - auc: 0.5011 - val_loss: 0.6948 - val_auc: 0.4931\n",
            "Epoch 2/5\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.6935 - auc: 0.5031 - val_loss: 0.6932 - val_auc: 0.5000\n",
            "Epoch 3/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6930 - auc: 0.5048 - val_loss: 0.6929 - val_auc: 0.5140\n",
            "Epoch 4/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6929 - auc: 0.5202 - val_loss: 0.6929 - val_auc: 0.5129\n",
            "Epoch 5/5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6928 - auc: 0.5165 - val_loss: 0.6927 - val_auc: 0.5390\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4dd44c15f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGbDX8c3PxUt"
      },
      "source": [
        "## 2.5 Learning Rate Schedular\n",
        "\n",
        " you have to decay learning based on below conditions\n",
        " \n",
        "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
        "               learning rate by 10%. \n",
        "        Cond2. For every 3rd epoch, decay your learning rate by 5%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K97BU_X6Pwmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1479
        },
        "outputId": "bb6f8f79-cd88-46df-cf9f-bf827afaf0c8"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def changeLearningRate(epoch):\n",
        "    initial_learningrate=0.01\n",
        "    if epoch % 3 ==0:\n",
        "      changed = initial_learningrate*(1-0.05)**epoch\n",
        "    else:\n",
        "      changed = initial_learningrate*(1-0.1)**epoch\n",
        "    return changed\n",
        "\n",
        "changed_lr = []\n",
        "for i in range(1,10):\n",
        "  changed_lr.append(changeLearningRate(i))\n",
        "  \n",
        "\n",
        "\n",
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(layer1)\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "lrschedule = LearningRateScheduler(changeLearningRate, verbose=0.1)\n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_auc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_auc',  verbose=1, save_best_only=True, mode='max')\n",
        "earlystop = EarlyStopping(monitor='val_auc', patience=2, verbose=1, mode='max')\n",
        "\n",
        "# here we are creating a list with all the callbacks we want\n",
        "callback_list = [history_own,lrschedule, earlystop, checkpoint,terminate]\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=20,callbacks=[callback_list])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/20\n",
            "669/670 [============================>.] - ETA: 0s - loss: 0.7250 - auc: 0.5025  f1_score: 0.4965\n",
            "\n",
            "Epoch 00001: val_auc improved from -inf to 0.49247, saving model to model_save/weights-01-0.4925.hdf5\n",
            "670/670 [==============================] - 2s 2ms/step - loss: 0.7249 - auc: 0.5025 - val_loss: 0.6964 - val_auc: 0.4925 - lr: 0.0100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009000000000000001.\n",
            "Epoch 2/20\n",
            "648/670 [============================>.] - ETA: 0s - loss: 0.6930 - auc: 0.5093  f1_score: 0.5159\n",
            "\n",
            "Epoch 00002: val_auc improved from 0.49247 to 0.52159, saving model to model_save/weights-02-0.5216.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6930 - auc: 0.5092 - val_loss: 0.6922 - val_auc: 0.5216 - lr: 0.0090\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.008100000000000001.\n",
            "Epoch 3/20\n",
            "641/670 [===========================>..] - ETA: 0s - loss: 0.6916 - auc: 0.5555  f1_score: 0.5826\n",
            "\n",
            "Epoch 00003: val_auc improved from 0.52159 to 0.57612, saving model to model_save/weights-03-0.5761.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6916 - auc: 0.5566 - val_loss: 0.6915 - val_auc: 0.5761 - lr: 0.0081\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00857375.\n",
            "Epoch 4/20\n",
            "651/670 [============================>.] - ETA: 0s - loss: 0.6909 - auc: 0.5872  f1_score: 0.5892\n",
            "\n",
            "Epoch 00004: val_auc improved from 0.57612 to 0.59385, saving model to model_save/weights-04-0.5939.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6909 - auc: 0.5872 - val_loss: 0.6908 - val_auc: 0.5939 - lr: 0.0086\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006561.\n",
            "Epoch 5/20\n",
            "661/670 [============================>.] - ETA: 0s - loss: 0.6903 - auc: 0.5984  f1_score: 0.5868\n",
            "\n",
            "Epoch 00005: val_auc improved from 0.59385 to 0.59848, saving model to model_save/weights-05-0.5985.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6903 - auc: 0.5984 - val_loss: 0.6903 - val_auc: 0.5985 - lr: 0.0066\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.005904900000000001.\n",
            "Epoch 6/20\n",
            "635/670 [===========================>..] - ETA: 0s - loss: 0.6898 - auc: 0.6020  f1_score: 0.5780\n",
            "\n",
            "Epoch 00006: val_auc improved from 0.59848 to 0.59907, saving model to model_save/weights-06-0.5991.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6897 - auc: 0.6026 - val_loss: 0.6899 - val_auc: 0.5991 - lr: 0.0059\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.007350918906249998.\n",
            "Epoch 7/20\n",
            "640/670 [===========================>..] - ETA: 0s - loss: 0.6891 - auc: 0.5996  f1_score: 0.5752\n",
            "\n",
            "Epoch 00007: val_auc improved from 0.59907 to 0.60118, saving model to model_save/weights-07-0.6012.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6892 - auc: 0.5981 - val_loss: 0.6892 - val_auc: 0.6012 - lr: 0.0074\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004782969000000001.\n",
            "Epoch 8/20\n",
            "651/670 [============================>.] - ETA: 0s - loss: 0.6886 - auc: 0.6134  f1_score: 0.5806\n",
            "\n",
            "Epoch 00008: val_auc improved from 0.60118 to 0.60611, saving model to model_save/weights-08-0.6061.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6886 - auc: 0.6129 - val_loss: 0.6888 - val_auc: 0.6061 - lr: 0.0048\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.004304672100000001.\n",
            "Epoch 9/20\n",
            "631/670 [===========================>..] - ETA: 0s - loss: 0.6882 - auc: 0.6142  f1_score: 0.5841\n",
            "\n",
            "Epoch 00009: val_auc did not improve from 0.60611\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6882 - auc: 0.6136 - val_loss: 0.6884 - val_auc: 0.6050 - lr: 0.0043\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.006302494097246091.\n",
            "Epoch 10/20\n",
            "640/670 [===========================>..] - ETA: 0s - loss: 0.6877 - auc: 0.6131  f1_score: 0.5832\n",
            "\n",
            "Epoch 00010: val_auc improved from 0.60611 to 0.60689, saving model to model_save/weights-10-0.6069.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6877 - auc: 0.6128 - val_loss: 0.6878 - val_auc: 0.6069 - lr: 0.0063\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.003486784401000001.\n",
            "Epoch 11/20\n",
            "644/670 [===========================>..] - ETA: 0s - loss: 0.6870 - auc: 0.6171  f1_score: 0.5809\n",
            "\n",
            "Epoch 00011: val_auc did not improve from 0.60689\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6871 - auc: 0.6154 - val_loss: 0.6875 - val_auc: 0.6053 - lr: 0.0035\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0031381059609000006.\n",
            "Epoch 12/20\n",
            "626/670 [===========================>..] - ETA: 0s - loss: 0.6867 - auc: 0.6166  f1_score: 0.5803\n",
            "\n",
            "Epoch 00012: val_auc did not improve from 0.60689\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6867 - auc: 0.6159 - val_loss: 0.6872 - val_auc: 0.6051 - lr: 0.0031\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4dd5dde1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51kT40pfFTza"
      },
      "source": [
        "## Observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAhxUKfLFZLJ"
      },
      "source": [
        "## 1. on epoch-4 f1_score is maximum-0.5892 --> val_auc= 0.5939\n",
        "## 2. Val_loss is decreasing on every epoch. Initially loss is decreasing significant but after few epoch loss change is not significant.\n",
        "##3. After epoch 10 val_auc is not increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DWQZnoUpepa"
      },
      "source": [
        "# Model-2\n",
        "\n",
        "1. Use relu as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use RandomUniform(0,1) as initilizer.\n",
        "3. Analyze your output and training process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOLgp1DvpzMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1122
        },
        "outputId": "d05b3bf6-ba53-40a3-8b4c-8dec94c8009d"
      },
      "source": [
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(0,1))(layer1)\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "lrschedule = LearningRateScheduler(changeLearningRate, verbose=0.1)\n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_auc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_auc',  verbose=1, save_best_only=True, mode='max')\n",
        "earlystop = EarlyStopping(monitor='val_auc', patience=2, verbose=1, mode='max')\n",
        "\n",
        "# here we are creating a list with all the callbacks we want\n",
        "callback_list = [history_own,lrschedule, earlystop, checkpoint,terminate]\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=20,callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/20\n",
            "656/670 [============================>.] - ETA: 0s - loss: 0.7357 - auc: 0.4643  f1_score: 0.4562\n",
            "\n",
            "Epoch 00001: val_auc improved from -inf to 0.45563, saving model to model_save/weights-01-0.4556.hdf5\n",
            "670/670 [==============================] - 2s 2ms/step - loss: 0.7352 - auc: 0.4640 - val_loss: 0.7080 - val_auc: 0.4556 - lr: 0.0100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009000000000000001.\n",
            "Epoch 2/20\n",
            "650/670 [============================>.] - ETA: 0s - loss: 0.7017 - auc: 0.4626  f1_score: 0.4409\n",
            "\n",
            "Epoch 00002: val_auc did not improve from 0.45563\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.7016 - auc: 0.4620 - val_loss: 0.6979 - val_auc: 0.4517 - lr: 0.0090\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.008100000000000001.\n",
            "Epoch 3/20\n",
            "647/670 [===========================>..] - ETA: 0s - loss: 0.6955 - auc: 0.4580  f1_score: 0.4935\n",
            "\n",
            "Epoch 00003: val_auc improved from 0.45563 to 0.45902, saving model to model_save/weights-03-0.4590.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6954 - auc: 0.4579 - val_loss: 0.6939 - val_auc: 0.4590 - lr: 0.0081\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00857375.\n",
            "Epoch 4/20\n",
            "647/670 [===========================>..] - ETA: 0s - loss: 0.6930 - auc: 0.5074  f1_score: 0.5065\n",
            "\n",
            "Epoch 00004: val_auc improved from 0.45902 to 0.52474, saving model to model_save/weights-04-0.5247.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6929 - auc: 0.5101 - val_loss: 0.6919 - val_auc: 0.5247 - lr: 0.0086\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006561.\n",
            "Epoch 5/20\n",
            "631/670 [===========================>..] - ETA: 0s - loss: 0.6917 - auc: 0.5262  f1_score: 0.5112\n",
            "\n",
            "Epoch 00005: val_auc improved from 0.52474 to 0.53018, saving model to model_save/weights-05-0.5302.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6916 - auc: 0.5267 - val_loss: 0.6909 - val_auc: 0.5302 - lr: 0.0066\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.005904900000000001.\n",
            "Epoch 6/20\n",
            "648/670 [============================>.] - ETA: 0s - loss: 0.6908 - auc: 0.5280  f1_score: 0.5123\n",
            "\n",
            "Epoch 00006: val_auc improved from 0.53018 to 0.53118, saving model to model_save/weights-06-0.5312.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6909 - auc: 0.5272 - val_loss: 0.6903 - val_auc: 0.5312 - lr: 0.0059\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.007350918906249998.\n",
            "Epoch 7/20\n",
            "631/670 [===========================>..] - ETA: 0s - loss: 0.6903 - auc: 0.5307  f1_score: 0.5156\n",
            "\n",
            "Epoch 00007: val_auc improved from 0.53118 to 0.53256, saving model to model_save/weights-07-0.5326.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6903 - auc: 0.5297 - val_loss: 0.6896 - val_auc: 0.5326 - lr: 0.0074\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004782969000000001.\n",
            "Epoch 8/20\n",
            "641/670 [===========================>..] - ETA: 0s - loss: 0.6901 - auc: 0.5264  f1_score: 0.5164\n",
            "\n",
            "Epoch 00008: val_auc did not improve from 0.53256\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6899 - auc: 0.5282 - val_loss: 0.6893 - val_auc: 0.5321 - lr: 0.0048\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.004304672100000001.\n",
            "Epoch 9/20\n",
            "666/670 [============================>.] - ETA: 0s - loss: 0.6896 - auc: 0.5274  f1_score: 0.5150\n",
            "\n",
            "Epoch 00009: val_auc did not improve from 0.53256\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6896 - auc: 0.5273 - val_loss: 0.6890 - val_auc: 0.5324 - lr: 0.0043\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4dce29ac18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbA-MGL1HXxT"
      },
      "source": [
        "# Observations\n",
        "\n",
        "## 1. f1_score is increasing on every epoch - max -f1_score= 0.5164 , val_auc= 0.5321\n",
        "## 2. Val_loss change is very less (not significant) on every epoch. \n",
        "## 3. After epoch 7 val_auc is not increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCmuxyPLtg0S"
      },
      "source": [
        "# Model-3\n",
        "1. Use relu as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use he_uniform() as initilizer.\n",
        "3. Analyze your output and training process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNaMhJ5gtqMd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2414
        },
        "outputId": "4942dd2d-c36c-4656-ec34-17ca65a97884"
      },
      "source": [
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform())(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.he_uniform())(layer1)\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "lrschedule = LearningRateScheduler(changeLearningRate, verbose=0.1)\n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_auc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_auc',  verbose=1, save_best_only=True, mode='max')\n",
        "earlystop = EarlyStopping(monitor='val_auc', patience=2, verbose=1, mode='max')\n",
        "\n",
        "# here we are creating a list with all the callbacks we want\n",
        "callback_list = [history_own,lrschedule, earlystop, checkpoint,terminate]\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=20,callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/20\n",
            "656/670 [============================>.] - ETA: 0s - loss: 0.7529 - auc: 0.5103  f1_score: 0.5164\n",
            "\n",
            "Epoch 00001: val_auc improved from -inf to 0.51928, saving model to model_save/weights-01-0.5193.hdf5\n",
            "670/670 [==============================] - 2s 2ms/step - loss: 0.7524 - auc: 0.5094 - val_loss: 0.7034 - val_auc: 0.5193 - lr: 0.0100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009000000000000001.\n",
            "Epoch 2/20\n",
            "638/670 [===========================>..] - ETA: 0s - loss: 0.6968 - auc: 0.5218  f1_score: 0.5185\n",
            "\n",
            "Epoch 00002: val_auc improved from 0.51928 to 0.53082, saving model to model_save/weights-02-0.5308.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6964 - auc: 0.5225 - val_loss: 0.6899 - val_auc: 0.5308 - lr: 0.0090\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.008100000000000001.\n",
            "Epoch 3/20\n",
            "660/670 [============================>.] - ETA: 0s - loss: 0.6879 - auc: 0.5390  f1_score: 0.5273\n",
            "\n",
            "Epoch 00003: val_auc improved from 0.53082 to 0.54588, saving model to model_save/weights-03-0.5459.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6880 - auc: 0.5392 - val_loss: 0.6863 - val_auc: 0.5459 - lr: 0.0081\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00857375.\n",
            "Epoch 4/20\n",
            "638/670 [===========================>..] - ETA: 0s - loss: 0.6847 - auc: 0.5546  f1_score: 0.5356\n",
            "\n",
            "Epoch 00004: val_auc improved from 0.54588 to 0.56316, saving model to model_save/weights-04-0.5632.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6848 - auc: 0.5556 - val_loss: 0.6845 - val_auc: 0.5632 - lr: 0.0086\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006561.\n",
            "Epoch 5/20\n",
            "666/670 [============================>.] - ETA: 0s - loss: 0.6828 - auc: 0.5697  f1_score: 0.5465\n",
            "\n",
            "Epoch 00005: val_auc improved from 0.56316 to 0.56531, saving model to model_save/weights-05-0.5653.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6828 - auc: 0.5696 - val_loss: 0.6833 - val_auc: 0.5653 - lr: 0.0066\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.005904900000000001.\n",
            "Epoch 6/20\n",
            "647/670 [===========================>..] - ETA: 0s - loss: 0.6813 - auc: 0.5760  f1_score: 0.5485\n",
            "\n",
            "Epoch 00006: val_auc improved from 0.56531 to 0.57148, saving model to model_save/weights-06-0.5715.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6814 - auc: 0.5752 - val_loss: 0.6823 - val_auc: 0.5715 - lr: 0.0059\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.007350918906249998.\n",
            "Epoch 7/20\n",
            "654/670 [============================>.] - ETA: 0s - loss: 0.6801 - auc: 0.5805  f1_score: 0.5488\n",
            "\n",
            "Epoch 00007: val_auc improved from 0.57148 to 0.57848, saving model to model_save/weights-07-0.5785.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6801 - auc: 0.5804 - val_loss: 0.6811 - val_auc: 0.5785 - lr: 0.0074\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004782969000000001.\n",
            "Epoch 8/20\n",
            "635/670 [===========================>..] - ETA: 0s - loss: 0.6789 - auc: 0.5862  f1_score: 0.5515\n",
            "\n",
            "Epoch 00008: val_auc improved from 0.57848 to 0.58206, saving model to model_save/weights-08-0.5821.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6789 - auc: 0.5861 - val_loss: 0.6803 - val_auc: 0.5821 - lr: 0.0048\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.004304672100000001.\n",
            "Epoch 9/20\n",
            "651/670 [============================>.] - ETA: 0s - loss: 0.6780 - auc: 0.5897  f1_score: 0.5515\n",
            "\n",
            "Epoch 00009: val_auc improved from 0.58206 to 0.58320, saving model to model_save/weights-09-0.5832.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6780 - auc: 0.5902 - val_loss: 0.6795 - val_auc: 0.5832 - lr: 0.0043\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.006302494097246091.\n",
            "Epoch 10/20\n",
            "666/670 [============================>.] - ETA: 0s - loss: 0.6770 - auc: 0.5932  f1_score: 0.5538\n",
            "\n",
            "Epoch 00010: val_auc improved from 0.58320 to 0.58773, saving model to model_save/weights-10-0.5877.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6770 - auc: 0.5932 - val_loss: 0.6783 - val_auc: 0.5877 - lr: 0.0063\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.003486784401000001.\n",
            "Epoch 11/20\n",
            "661/670 [============================>.] - ETA: 0s - loss: 0.6760 - auc: 0.5975  f1_score: 0.5555\n",
            "\n",
            "Epoch 00011: val_auc improved from 0.58773 to 0.59102, saving model to model_save/weights-11-0.5910.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6759 - auc: 0.5979 - val_loss: 0.6776 - val_auc: 0.5910 - lr: 0.0035\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0031381059609000006.\n",
            "Epoch 12/20\n",
            "659/670 [============================>.] - ETA: 0s - loss: 0.6750 - auc: 0.6013  f1_score: 0.5558\n",
            "\n",
            "Epoch 00012: val_auc improved from 0.59102 to 0.59363, saving model to model_save/weights-12-0.5936.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6751 - auc: 0.6007 - val_loss: 0.6770 - val_auc: 0.5936 - lr: 0.0031\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.005403600876626367.\n",
            "Epoch 13/20\n",
            "661/670 [============================>.] - ETA: 0s - loss: 0.6740 - auc: 0.6052  f1_score: 0.5570\n",
            "\n",
            "Epoch 00013: val_auc improved from 0.59363 to 0.59802, saving model to model_save/weights-13-0.5980.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6742 - auc: 0.6045 - val_loss: 0.6758 - val_auc: 0.5980 - lr: 0.0054\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.002541865828329001.\n",
            "Epoch 14/20\n",
            "668/670 [============================>.] - ETA: 0s - loss: 0.6732 - auc: 0.6075  f1_score: 0.5574\n",
            "\n",
            "Epoch 00014: val_auc improved from 0.59802 to 0.60003, saving model to model_save/weights-14-0.6000.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6732 - auc: 0.6075 - val_loss: 0.6753 - val_auc: 0.6000 - lr: 0.0025\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.002287679245496101.\n",
            "Epoch 15/20\n",
            "663/670 [============================>.] - ETA: 0s - loss: 0.6726 - auc: 0.6099  f1_score: 0.5570\n",
            "\n",
            "Epoch 00015: val_auc improved from 0.60003 to 0.60212, saving model to model_save/weights-15-0.6021.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6726 - auc: 0.6097 - val_loss: 0.6748 - val_auc: 0.6021 - lr: 0.0023\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00463291230159753.\n",
            "Epoch 16/20\n",
            "644/670 [===========================>..] - ETA: 0s - loss: 0.6717 - auc: 0.6134  f1_score: 0.5589\n",
            "\n",
            "Epoch 00016: val_auc improved from 0.60212 to 0.60674, saving model to model_save/weights-16-0.6067.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6718 - auc: 0.6122 - val_loss: 0.6738 - val_auc: 0.6067 - lr: 0.0046\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0018530201888518416.\n",
            "Epoch 17/20\n",
            "636/670 [===========================>..] - ETA: 0s - loss: 0.6707 - auc: 0.6178  f1_score: 0.5603\n",
            "\n",
            "Epoch 00017: val_auc improved from 0.60674 to 0.60814, saving model to model_save/weights-17-0.6081.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6709 - auc: 0.6167 - val_loss: 0.6733 - val_auc: 0.6081 - lr: 0.0019\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0016677181699666576.\n",
            "Epoch 18/20\n",
            "644/670 [===========================>..] - ETA: 0s - loss: 0.6703 - auc: 0.6189  f1_score: 0.5614\n",
            "\n",
            "Epoch 00018: val_auc improved from 0.60814 to 0.60945, saving model to model_save/weights-18-0.6095.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6704 - auc: 0.6185 - val_loss: 0.6728 - val_auc: 0.6095 - lr: 0.0017\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.003972143184582182.\n",
            "Epoch 19/20\n",
            "653/670 [============================>.] - ETA: 0s - loss: 0.6695 - auc: 0.6217  f1_score: 0.5638\n",
            "\n",
            "Epoch 00019: val_auc improved from 0.60945 to 0.61259, saving model to model_save/weights-19-0.6126.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6697 - auc: 0.6213 - val_loss: 0.6718 - val_auc: 0.6126 - lr: 0.0040\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0013508517176729928.\n",
            "Epoch 20/20\n",
            "639/670 [===========================>..] - ETA: 0s - loss: 0.6687 - auc: 0.6242  f1_score: 0.5639\n",
            "\n",
            "Epoch 00020: val_auc improved from 0.61259 to 0.61408, saving model to model_save/weights-20-0.6141.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6689 - auc: 0.6237 - val_loss: 0.6714 - val_auc: 0.6141 - lr: 0.0014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4dcf8f22b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx3R8k_2I2OR"
      },
      "source": [
        "# Observation\n",
        "\n",
        "## 1. f1_score and val_auc is increasing on every epoch. max -f1_score = 0.5639  val_auc= 0.6141\n",
        "## 2. Val_loss is decreasing on every epoch. Initially loss is decreasing significant but after few epoch loss change is not significant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deuGSaNoKOVa"
      },
      "source": [
        "# Model-4\n",
        "\n",
        "1. Try with any values to get better accuracy/f1 score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pye46n3hKbRp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2414
        },
        "outputId": "73098f0a-c8ce-4375-c6f5-27e627931a16"
      },
      "source": [
        "#Input layer\n",
        "input_layer = Input(shape=(2,))\n",
        "#Dense hidden layer\n",
        "layer1 = Dense(5,activation='selu',kernel_initializer=tf.keras.initializers.he_uniform())(input_layer)\n",
        "#output layer\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.he_uniform())(layer1)\n",
        "#Creating a model\n",
        "model = Model(inputs=input_layer,outputs=output)\n",
        "\n",
        "lrschedule = LearningRateScheduler(changeLearningRate, verbose=0.1)\n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_auc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_auc',  verbose=1, save_best_only=True, mode='max')\n",
        "earlystop = EarlyStopping(monitor='val_auc', patience=2, verbose=1, mode='max')\n",
        "\n",
        "# here we are creating a list with all the callbacks we want\n",
        "callback_list = [history_own,lrschedule, earlystop, checkpoint,terminate]\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='BinaryCrossentropy',metrics=['AUC'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test),batch_size=20,callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/20\n",
            "652/670 [============================>.] - ETA: 0s - loss: 0.6896 - auc: 0.5760  f1_score: 0.6114\n",
            "\n",
            "Epoch 00001: val_auc improved from -inf to 0.66031, saving model to model_save/weights-01-0.6603.hdf5\n",
            "670/670 [==============================] - 2s 2ms/step - loss: 0.6893 - auc: 0.5773 - val_loss: 0.6751 - val_auc: 0.6603 - lr: 0.0100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009000000000000001.\n",
            "Epoch 2/20\n",
            "653/670 [============================>.] - ETA: 0s - loss: 0.6730 - auc: 0.6596  f1_score: 0.6058\n",
            "\n",
            "Epoch 00002: val_auc did not improve from 0.66031\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6729 - auc: 0.6598 - val_loss: 0.6719 - val_auc: 0.6591 - lr: 0.0090\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.008100000000000001.\n",
            "Epoch 3/20\n",
            "664/670 [============================>.] - ETA: 0s - loss: 0.6694 - auc: 0.6595  f1_score: 0.6089\n",
            "\n",
            "Epoch 00003: val_auc improved from 0.66031 to 0.66114, saving model to model_save/weights-03-0.6611.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6695 - auc: 0.6588 - val_loss: 0.6690 - val_auc: 0.6611 - lr: 0.0081\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00857375.\n",
            "Epoch 4/20\n",
            "664/670 [============================>.] - ETA: 0s - loss: 0.6663 - auc: 0.6626  f1_score: 0.6100\n",
            "\n",
            "Epoch 00004: val_auc did not improve from 0.66114\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6662 - auc: 0.6627 - val_loss: 0.6667 - val_auc: 0.6540 - lr: 0.0086\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006561.\n",
            "Epoch 5/20\n",
            "657/670 [============================>.] - ETA: 0s - loss: 0.6636 - auc: 0.6640  f1_score: 0.6223\n",
            "\n",
            "Epoch 00005: val_auc improved from 0.66114 to 0.66387, saving model to model_save/weights-05-0.6639.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6634 - auc: 0.6644 - val_loss: 0.6638 - val_auc: 0.6639 - lr: 0.0066\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.005904900000000001.\n",
            "Epoch 6/20\n",
            "653/670 [============================>.] - ETA: 0s - loss: 0.6613 - auc: 0.6678  f1_score: 0.6120\n",
            "\n",
            "Epoch 00006: val_auc improved from 0.66387 to 0.66752, saving model to model_save/weights-06-0.6675.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6612 - auc: 0.6681 - val_loss: 0.6618 - val_auc: 0.6675 - lr: 0.0059\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.007350918906249998.\n",
            "Epoch 7/20\n",
            "668/670 [============================>.] - ETA: 0s - loss: 0.6589 - auc: 0.6719  f1_score: 0.6218\n",
            "\n",
            "Epoch 00007: val_auc did not improve from 0.66752\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6588 - auc: 0.6722 - val_loss: 0.6596 - val_auc: 0.6651 - lr: 0.0074\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004782969000000001.\n",
            "Epoch 8/20\n",
            "670/670 [==============================] - ETA: 0s - loss: 0.6565 - auc: 0.6742  f1_score: 0.6248\n",
            "\n",
            "Epoch 00008: val_auc improved from 0.66752 to 0.67084, saving model to model_save/weights-08-0.6708.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6565 - auc: 0.6742 - val_loss: 0.6576 - val_auc: 0.6708 - lr: 0.0048\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.004304672100000001.\n",
            "Epoch 9/20\n",
            "659/670 [============================>.] - ETA: 0s - loss: 0.6548 - auc: 0.6771  f1_score: 0.6253\n",
            "\n",
            "Epoch 00009: val_auc improved from 0.67084 to 0.67276, saving model to model_save/weights-09-0.6728.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6546 - auc: 0.6777 - val_loss: 0.6561 - val_auc: 0.6728 - lr: 0.0043\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.006302494097246091.\n",
            "Epoch 10/20\n",
            "651/670 [============================>.] - ETA: 0s - loss: 0.6526 - auc: 0.6808  f1_score: 0.6250\n",
            "\n",
            "Epoch 00010: val_auc improved from 0.67276 to 0.67787, saving model to model_save/weights-10-0.6779.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6525 - auc: 0.6810 - val_loss: 0.6538 - val_auc: 0.6779 - lr: 0.0063\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.003486784401000001.\n",
            "Epoch 11/20\n",
            "665/670 [============================>.] - ETA: 0s - loss: 0.6505 - auc: 0.6842  f1_score: 0.6286\n",
            "\n",
            "Epoch 00011: val_auc improved from 0.67787 to 0.68398, saving model to model_save/weights-11-0.6840.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6505 - auc: 0.6841 - val_loss: 0.6519 - val_auc: 0.6840 - lr: 0.0035\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0031381059609000006.\n",
            "Epoch 12/20\n",
            "645/670 [===========================>..] - ETA: 0s - loss: 0.6491 - auc: 0.6883  f1_score: 0.6283\n",
            "\n",
            "Epoch 00012: val_auc improved from 0.68398 to 0.68556, saving model to model_save/weights-12-0.6856.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6490 - auc: 0.6882 - val_loss: 0.6507 - val_auc: 0.6856 - lr: 0.0031\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.005403600876626367.\n",
            "Epoch 13/20\n",
            "644/670 [===========================>..] - ETA: 0s - loss: 0.6472 - auc: 0.6910  f1_score: 0.6302\n",
            "\n",
            "Epoch 00013: val_auc improved from 0.68556 to 0.68960, saving model to model_save/weights-13-0.6896.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6472 - auc: 0.6911 - val_loss: 0.6485 - val_auc: 0.6896 - lr: 0.0054\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.002541865828329001.\n",
            "Epoch 14/20\n",
            "634/670 [===========================>..] - ETA: 0s - loss: 0.6457 - auc: 0.6936  f1_score: 0.6324\n",
            "\n",
            "Epoch 00014: val_auc improved from 0.68960 to 0.69056, saving model to model_save/weights-14-0.6906.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6454 - auc: 0.6946 - val_loss: 0.6473 - val_auc: 0.6906 - lr: 0.0025\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.002287679245496101.\n",
            "Epoch 15/20\n",
            "651/670 [============================>.] - ETA: 0s - loss: 0.6440 - auc: 0.6966  f1_score: 0.6332\n",
            "\n",
            "Epoch 00015: val_auc improved from 0.69056 to 0.69374, saving model to model_save/weights-15-0.6937.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6443 - auc: 0.6954 - val_loss: 0.6461 - val_auc: 0.6937 - lr: 0.0023\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00463291230159753.\n",
            "Epoch 16/20\n",
            "641/670 [===========================>..] - ETA: 0s - loss: 0.6426 - auc: 0.6991  f1_score: 0.6359\n",
            "\n",
            "Epoch 00016: val_auc improved from 0.69374 to 0.69627, saving model to model_save/weights-16-0.6963.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6429 - auc: 0.6984 - val_loss: 0.6441 - val_auc: 0.6963 - lr: 0.0046\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0018530201888518416.\n",
            "Epoch 17/20\n",
            "657/670 [============================>.] - ETA: 0s - loss: 0.6407 - auc: 0.7024  f1_score: 0.6374\n",
            "\n",
            "Epoch 00017: val_auc improved from 0.69627 to 0.69835, saving model to model_save/weights-17-0.6983.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6412 - auc: 0.7012 - val_loss: 0.6431 - val_auc: 0.6983 - lr: 0.0019\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0016677181699666576.\n",
            "Epoch 18/20\n",
            "667/670 [============================>.] - ETA: 0s - loss: 0.6404 - auc: 0.7023  f1_score: 0.6379\n",
            "\n",
            "Epoch 00018: val_auc improved from 0.69835 to 0.69963, saving model to model_save/weights-18-0.6996.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6404 - auc: 0.7026 - val_loss: 0.6423 - val_auc: 0.6996 - lr: 0.0017\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.003972143184582182.\n",
            "Epoch 19/20\n",
            "665/670 [============================>.] - ETA: 0s - loss: 0.6392 - auc: 0.7042  f1_score: 0.6389\n",
            "\n",
            "Epoch 00019: val_auc improved from 0.69963 to 0.70156, saving model to model_save/weights-19-0.7016.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6391 - auc: 0.7044 - val_loss: 0.6406 - val_auc: 0.7016 - lr: 0.0040\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0013508517176729928.\n",
            "Epoch 20/20\n",
            "670/670 [==============================] - ETA: 0s - loss: 0.6378 - auc: 0.7062  f1_score: 0.6397\n",
            "\n",
            "Epoch 00020: val_auc improved from 0.70156 to 0.70320, saving model to model_save/weights-20-0.7032.hdf5\n",
            "670/670 [==============================] - 1s 2ms/step - loss: 0.6378 - auc: 0.7062 - val_loss: 0.6399 - val_auc: 0.7032 - lr: 0.0014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4dcf4d3438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-tksmovNEYq"
      },
      "source": [
        "# Observation\n",
        "\n",
        "## 1. Using 'selu' as activation layer val_auc is increasing at every epoch. max -val_acu =.7032 and f1_score = 0.6397\n",
        "\n",
        "##2 val_loss is decreasing on every epoch."
      ]
    }
  ]
}