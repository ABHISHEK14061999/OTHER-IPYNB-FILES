{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "# Standardizing the data.\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.08 seconds.\n",
      "Convergence after 10 epochs took 0.08 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
       "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
       "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
       " (1, 15),\n",
       " array([-0.8531383]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (dim,1) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w = np.zeros_like(dim, dtype=float)\n",
    "    \n",
    "    b = 0\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    \n",
    "    exp_val = np.exp(-z)\n",
    "    denominator = np.add(1, exp_val)\n",
    "    \n",
    "    sigmoid_val = np.divide(1, denominator)\n",
    "    \n",
    "    return sigmoid_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "\n",
    "    '''In this function, we will compute log loss '''\n",
    "    \n",
    "    entorpy = 0.0\n",
    "    for y, y_hat in zip(y_true, y_pred):\n",
    "        \n",
    "        log10_t = np.where(y_hat > 0.0000000001, y_hat, 0)\n",
    "        true_event = np.multiply(y, np.log10(log10_t, out = log10_t, where = log10_t > 0))\n",
    "        \n",
    "        log10_f = np.where((1 - y_hat) > 0.0000000001, (1 - y_hat), 0)\n",
    "        false_event = np.multiply((1 - y), np.log10(log10_f, out=log10_f, where = log10_f > 0))\n",
    "\n",
    "        entorpy += np.add(true_event, false_event)\n",
    "\n",
    "    loss = -np.divide(entorpy, len(y_true))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(loss==0.07644900402910389)\n",
    "  return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "\n",
    "    dw = x * (y - sigmoid(np.dot(w, x) + b)) - (alpha / N) * w\n",
    "\n",
    "\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  assert(np.sum(grad_dw)==2.613689585)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "def gradient_db(x,y,w,b):\n",
    "        '''In this function, we will compute gradient w.r.to b '''\n",
    "        db = y - sigmoid(np.dot(w, x) + b)\n",
    "\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(grad_db==-0.5)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "loss_tr=[]\n",
    "loss_ts=[]\n",
    "epoch_list = []\n",
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "\n",
    "    w,b = initialize_weights(X_train[0])\n",
    "    eta0 = 0.0001\n",
    "    \n",
    "    for e in tqdm(range(epochs)):\n",
    "        y_tr_pred=[]\n",
    "        y_ts_pred=[]\n",
    "        for i in range(len(X_train)):\n",
    "            \n",
    "            w = w + (eta0 * gradient_dw(X_train[i], y_train[i], w, b, alpha, N))\n",
    "            b = b + (eta0 * gradient_db(X_train[i], y_train[i], w, b))\n",
    "\n",
    "        for k in range(len(X_train)):\n",
    "            z = np.dot(w,X_train[k])+b\n",
    "            s = sigmoid(z)\n",
    "            y_tr_pred.append(s)\n",
    "        l_tr = logloss(y_train,y_tr_pred)\n",
    "        loss_tr.append(l_tr)\n",
    "\n",
    "        for j in range(len(X_test)):\n",
    "            z_test = np.dot(w,X_test[j])+b\n",
    "            s_test = sigmoid(z_test)\n",
    "            y_ts_pred.append(s_test) \n",
    "        l_ts = logloss(y_test,y_ts_pred)\n",
    "        loss_ts.append(l_ts)\n",
    "\n",
    "        epoch_list.append(e)\n",
    "        \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "37500\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:13<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8918925964701091"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.29755932e-01,  1.93023807e-01, -1.48464457e-01,  3.38103415e-01,\n",
       "       -2.21228940e-01,  5.69932597e-01, -4.45183638e-01, -8.99209785e-02,\n",
       "        2.21804834e-01,  1.73809448e-01,  1.98727704e-01, -5.59450918e-04,\n",
       "       -8.13106259e-02,  3.39094296e-01,  2.29784893e-02])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot((epoch_list), (loss_tr), label = \"Train\")\n",
    "plt.plot((epoch_list), (loss_ts), label = \"Test\")\n",
    "plt.title(\"Train vs Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00638902,  0.00754815,  0.0001259 , -0.00334065, -0.01304224,\n",
       "          0.00976681,  0.00724119,  0.00416715,  0.01253163, -0.00703181,\n",
       "          0.0016758 , -0.00477861, -0.00170693,  0.00056628,  0.00031128]]),\n",
       " array([-0.0387543]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95224\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
